means
}
columnmean(airquality)
columnmean(airquality,FALSE)
?sd()
args(lm)
f <- function(a, b) {
a^2
}
f(2)
f <- function(a,b) {
print(a)
print(b)
}
f(45)
mean
myplot <- function(x,y,type="l",...) {
plot(x,y,type=type,...)
}
myplot
args(paste)
args(cat)
args(paste)
paste("a","b",sep=":")
paste("a","b",se=":")
lm <- function(x) { x*x }
lm
search()
make.power <- function(n) {
pow <- function(x) {
x^n
}
pow
}
cube <- make.power(3)
square <- make.power(2)
cube(3)
square(3)
cube()
cubels(environment(cube))
ls(environment(cube))
get("n",environment(cube))
ls(environment(square))
get("n",environment(square))
y <- 10
f <- function(x) {
y <- 2
y^2 + g(x)
}
# y is a free variable
g <- function(x) {
x*y
}
f(3)
g <- function(x) {
a <- 3
x+a+y
}
g(2)
q()
g <- function(x) {
a <- 3
x+a+y
}
g(2)
y <- 3
g <- function(x) {
a <- 3
x+a+y
}
g(2)
make.NegLogLik <- function(data, fixed=c(FALSE, FALSE)) {
params <- fixed
function(p) {
params[!fixed] <- p
mu <- params[1]
sigma <- params[2]
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2) / (sigma^2)
-(a+b)
}
}
set.seed(1);
normas <- rnorm(100,1,2)
nLL <- make.NegLokLik(normals)
nLL
set.seed(1);
normas <- rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
nLL
ls(environment(nLL))
optim(c(mu = 0, sigma = 1), nLL)$par
set.seed(1);
normals <- rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
optim(c(mu = 0, sigma = 1), nLL)$par
nLL <- make.NegLogLik(normals, c(FALSE, 2))
optimize(nLL, c(-1, 3))$minimum
nLL <- make.NegLogLik(normals, c(1, FALSE))
optimize(nLL, c(1e-6, 10))$minimum
nLL <- make.NegLogLike(normals, c(1, FALSE))
x <- seq(1.7, 1.9, len=100)
y <- sapply(x, nLL)
plot(x, exp(-(y-min(y))), type = "l")
nLL <- make.NegLogLik(normals, c(1, FALSE))
x <- seq(1.7, 1.9, len=100)
y <- sapply(x, nLL)
plot(x, exp(-(y-min(y))), type = "l")
nLL <- make.NegLogLik(normals, c(FALSE, 2))
x <- seq(0.5, 1.5, len=100)
y <- sapply(x, nLL)
plot(x, exp(-(y-min(y))), type = "l")
x <- as.Date("1970-01-01")
x
unclass(x)
unclass(as.Date("1970-01-02"))
x<-Sys.time()
x
p <- as.POSIXlt(x)
names(unclass(p))
p$sec
x<-Sys.time()
x
unclass(x)
x$sec
p <- as.POSIXlt(x)
p$sec
datestring <- c("January 10, 2012 10:40", "December 9, 2011 9:10")
x <- strptime(datestring, "%B %d, %Y %H:%M")
x
class(x)
x <- as.Date("2012-01-01")
y <- strptime("9 Jan 2011 11:34:21", "%d %b %Y %H:%M:%S")
x-y
x <- as.POSIXlt(x)
x-y
x <- as.Date("2012-03-01")
y <- as.Date("2012-02-28")
x-y
x <- as.POSIXct("2012-10-25 01:00:00")
y <- as.POSIXct("2012-10-25 06:00:00", tz="GMT")
y-x
x <- as.POSIXct("2012-10-25 01:00:00")
y <- as.POSIXct("2012-10-25 06:00:00", tz="GMT")
y-
x
q()
0.88*0.93 / (0.88*0.95 + (1-0.88)*0.05)
0.88*0.93/(0.88*0.95 + (1-0.93)*0.05)
0.88*0.95/(0.88*0.95 + (1-0.93)*0.05)
?qnorm
qnorm(0.05, 100, 10)
qnorm(0.05, mean=100, sd=10)
qnorm(5, mean=100, sd=10)
qnorm(0.95, 100, 10)
qnorm(0.95, 100, sd/sqrt(50))
qnorm(0.95, 100, sd=sqrt(50))
qnorm(0.95, 100, sd=10/sqrt(50))
qnorm(0.95, 100, sd=10/50)
pbinom(4, 6, 0.5)
?pbinom
pbinom(4, 6, 0.5, lower.tail=FALSE)
?pnorm
pnorm(0.51, 0.5, sqrt(1/(12*100)), lower.tail=FALSE)
1*(1/6) + 4*(1/6) + 9*(1/6) + 16*(1/6) + 25*(1/6) + 36*(1/6)
- (1*(1/6))^2 - (2*(1/6))^2 - (3*(1/6))^2 - (4*(1/6))^2
- (5*(1/6))^2 - (6*(1/6))^2
1*(1/6) + 4*(1/6) + 9*(1/6) + 16*(1/6) + 25*(1/6) + 36*(1/6) - (1*(1/6))^2 - (2*(1/6))^2 - (3*(1/6))^2 - (4*(1/6))^2 - (5*(1/6))^2 - (6*(1/6))^2
p <- (1/6)
s <- 1*p + 2*p + 3*p + 4*p + 5*p + 6*p
s <- s^2
s
s1 <- 1*p + 4*p + 9*p + 16*p + 25*p + 36*p
s1-s
2.916667/10
ppois(20, 16.5*2)
pnorm(70, 80, 10)
qnorm(0.95, 1100, 75)
qnorm(0.95, 1100, 75/sqrt(100))
pbinom(3, 5, 0.5, lower.tail=FALSE)
10/sqrt(100)
ppois(10, 5*3)
library(UsingR)
data(diamond)
plot(diamond$carat, diamon$price,
xlab = "Mass (carats)",
ylab = "Prices (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21, frame = FALSE)
abline(lm(price ~ carat, data = diamond), lwd = 2)
plot(diamond$carat, diamond$price,
xlab = "Mass (carats)",
ylab = "Prices (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21, frame = FALSE)
abline(lm(price ~ carat, data = diamond), lwd = 2)
fit <- lm(price ~ carat, data = diamond)
coef(fit)
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(fit2)
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx)
)
data(diamond)
plot(diamond$carat, diamond$price,
xlab = "Mass (carats)",
ylab = "Price (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
points(diamond$carat, predict(fit), pch = 19, col = "red")
lines(c(0.16, 0.16, 0.12),
c(200, coef(fit)[1] + coef(fit)[2] * 0.16,
coef(fit)[1] + coef(fit)[2] * 0.16))
lines(c(0.27, 0.27, 0.12),
c(200, coef(fit)[1] + coef(fit)[2] * 0.27,
coef(fit)[1] + coef(fit)[2] * 0.27))
lines(c(0.34, 0.34, 0.12),
c(200, coef(fit)[1] + coef(fit)[2] * 0.34,
coef(fit)[1] + coef(fit)[2] * 0.34))
text(newx, rep(250, 3), labels = newx, pos = 2)
y <- diamond$price
x <- diamond$carat
n <- length(y)
fit <- lm(y~x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e - (y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
plot(diamond$carat, diamond$price,
xlab = "Mass (carats)",
ylab = "Price (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
for (i in 1 : n)
lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red" , lwd = 2)
plot(diamond$carat, e,
xlab = "Mass (carats)",
ylab = "Residuals (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n)
lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 2)
x <- runif(100, -3, 3)
y <- x+sin(x) + rnorm(100, sd=0.2)
plot(x,y)
abline(lm(y~x))
plot(x, resid(lm(y~x)))
abline(h=0)
x <- runif(100, 0, 6)
y <- x + rnorm(100, mean=0, sd=0.001*x)
plot(x,y)
abline(lm(y~x))
plot(x, resid(lm(y~x)))
abline(h=0)
q()
library(UsingR)
data(diamond)
y <- diamond$price
x <- diamond$carat
n <- length(y)
fit <- lm(y~x)
summary(fit)$sigma
sqrt(sum(resid(fit)^2) / (n-2))
require(stats); require(graphics); data(anscombe)
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
## or   ff[[2]] <- as.name(paste0("y", i))
##      ff[[3]] <- as.name(paste0("x", i))
mods[[i]] <- lmi <- lm(ff, data = anscombe)
#print(anova(lmi))
}
## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
xlim = c(3, 19), ylim = c(3, 13))
abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
y <- diamond$price
x <- diamond$carat
n <- length(y)
beta1 <- cor(y,x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1*mean(x)
e <- y - beta0 - beta1*x
sigam <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x-mean(x))^2)
seBeta0 <- (1/n + mean(x)^2 / ssx)^0.5 * sigma
seBeta1 <- sigam / sqrt(ssx)
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x-mean(x))^2)
seBeta0 <- (1/n + mean(x)^2 / ssx)^0.5 * sigma
seBeta1 <- sigam / sqrt(ssx)
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n-2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n-2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std.Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercpet)","x")
coefTable
fit <- lm(y~x)
summary(fit)$coefficients
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1,1) * qt(0.975, df=fit$df) * sumCoef[1,2]
sumCoef[2,1] + c(-1,1) * qt(0.975, df=fit$df) * sumCoef[2,2]
plot(x,y, frame=FALSE, xlab = "Carat", ylab="Dollars", pch=21, col="black",
bg="lightblue", cex=1)
abline(fit, lwd=2)
xVals <- seq(min(x), max(x), by = 0.01)
yVals <- beta0 + beta1 * xVals
se1 <- sigma * sqrt(1/n + (xVals - mean(x))^2/ssx)
se2 <- sigma * sqrt(1 + 1/n + (xVals - mean(x))^2/ssx)
lines(xVals, yVals + 2*se1)
lines(xVals, yVals - 2*se1)
lines(xVals, yVals + 2*se2)
lines(xVals, yVals - 2*se2)
newdata <- data.frame(x = xVals)
p1 <- predict(fit, newdata, interval = ("confidence"))
p1 <- predict(fit, newdata, interval = ("prediction"))
plot(x,y,frame=FALSE, xlab="Carat", ylab = "Dollars", pch=21, col="black",
bg="lightblue", cex=2)
abline(fit, lwd=2)
lines(xVals, p1[,2]); lines(xVals, p1[,3])
lines(xVals, p2[,2]); lines(xVals, p2[,3])
newdata <- data.frame(x = xVals)
p1 <- predict(fit, newdata, interval = ("confidence"))
p2 <- predict(fit, newdata, interval = ("prediction"))
plot(x,y,frame=FALSE, xlab="Carat", ylab = "Dollars", pch=21, col="black",
bg="lightblue", cex=2)
abline(fit, lwd=2)
lines(xVals, p1[,2]); lines(xVals, p1[,3])
lines(xVals, p2[,2]); lines(xVals, p2[,3])
q()
n <- 100
x <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
y <- x + x2 + x3 + rnorm(n, sd=0.1)
e <- function(a,b) a - sum(a*b) / sum(b^2) * b
ey <- e(e(y,x2), e(x3,x2))
ex <- e(e(x,x2), e(x3,x2))
sum(ey*ex) / sum(ex^2)
coef(lm(y~x+x2+x3-1))
ey <- e(e(y,x3), e(x2,x3))
ex <- e(e(x,x3), e(x2,x3))
# Regression through the origin slope
sum(ey*ex) / sum(ex^2)
ey <- resid(lm(y~x2+x3-1))
ex <- resid(lm(x~x2+x3-1))
sum(ey * ex) / sum(ex^2)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(data)
head(concrete)
str(concrete)
x <- 1:1030
plot(x,concrete$CompressiveStrength)
str(training)
x <- 1:774
plot(x,training$CompressiveStrength)
qplot(y=training$CompressiveStrength,colour=training$Age)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
summary(training)
trainIL <- training[58:69]
summary(trainIL)
?preProcess
preProcess(trainIL,thresh=0.90,method="pca")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainIL1 <- training[58:69]
trainD <- training[1]
summary(trainD)
trainIL <- cbind(trainD,trainIL1)
summary(trainIL)
?train
train(trainIL,method="glm")
train(trainIL[-1,],trainIL[1],method="glm")
train(trainIL[,-1],trainIL[,1],method="glm")
train(diagnosis~.,method="glm",data=trainIL)
t1 <- train(trainIL[,-1],trainIL[,1],method="glm")
?predict
p1 <- predict(t1,testing)
p1
confusionMatrix(p1,testing$diagnosis)
preprocessIL <- preProcess(trainIL[,-1],method="pca",thresh=0.80)
t2 <- train(preprocessIL[,-1],preprocessIL[,1],method="glm")
preprocessIL2 <- predict(preprocessIL,trainIL[,-1])
summary(preprocessIL2)
t2 <- train(trainIL[,1]~.,method="glm",data=preprocessIL2)
p2 <- predict(t2,testing)
p2 <- predict(preprocessIL,testing)
p2 <- predict(preprocessIL2,testing)
t2 <- predict(preprocessIL,trainIL[,-1])
t22 <- train(trainIL[,1] ~., method="glm",data=t2)
p2 <- predict(preprocessIL,testing[,-1])
testIL <- testing[58:69]
p2 <- predict(preprocessIL,testIL)
confusionMatrix(testing[,1],predict(t22,p2))
?inv
??inv
makeVector <- function(x = numeric()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setmean <- function(mean) m <<- mean
getmean <- function() m
list(set = set, get = get,
setmean = setmean,
getmean = getmean)
}
cachemean <- function(x, ...) {
m <- x$getmean()
if(!is.null(m)) {
message("getting cached data")
return(m)
}
data <- x$get()
m <- mean(data, ...)
x$setmean(m)
m
}
x <- 1:50
cachemean(x)
makeVector()
x
cachemean(x)
bigVec <- makeVector(1:1000)
cachemean(bigVec)
cachemean(bigVec)
biggerVec <- makeVector(1:100000)
cachemean(biggerVec)
cachemean(biggerVec)
setwd("testR/Programming_in_R_Coursera/Project/")
source("cachematrix.R")
dir()
setwd("ProgrammingAssignment2/")
source("cachematrix.R")
amatrix = makeCacheMatrix(matrix(c(1,2,3,4), nrow=2, ncol=2))
amatrix$get()
cacheSolve(amatrix)
amatrix$getinverse()
cacheSolve(amatrix)
amatrix$set(matrix(c(0,5,99,66), nrow=2, ncol=2))
cacheSolve(amatrix)
amatrix$get()
amatrix$getinverse()
cacheSolve(amatrix)
a <- makeVector(c(1,2,3,4))
a$get()
a$getmean()
cachemean(a)
a$getmean()
cachemean(a)
a$set(c(10,20,30,40))
a$getmean()
cachemean(a)
cachemean(a)
a$get()
a$setmean(0)
a$getmean()
a$get()
cachemean(a)
a <- makeVector(c(5, 25, 125, 625))
a$get()
cachemean(a)
cachemean(a)
q()
